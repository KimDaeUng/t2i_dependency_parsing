{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing CUB, Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch == 1.1.0\n",
    "from glob import glob\n",
    "import os\n",
    "import pprint\n",
    "import stanfordnlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default download directory: C:\\Users\\dwkim\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\dwkim\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 235M/235M [01:05<00:00, 3.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\dwkim\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross',\n",
      " '002.Laysan_Albatross',\n",
      " '003.Sooty_Albatross',\n",
      " '004.Groove_billed_Ani',\n",
      " '005.Crested_Auklet']\n",
      "['.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0002_55.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0003_796136.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0005_796090.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0006_796065.txt']\n"
     ]
    }
   ],
   "source": [
    "# Origin path\n",
    "orig_dir_path = \".\\\\text_c10\"\n",
    "orig_text_path = \".\\\\text_c10\\\\*\\\\*.txt\"\n",
    "orig_dir_list = os.listdir(orig_dir_path)\n",
    "orig_text_list = glob(orig_text_path)\n",
    "pprint.pprint(orig_dir_list[:5])\n",
    "pprint.pprint(orig_text_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\text_c10_convert\\\\001.Black_footed_Albatross',\n",
      " '.\\\\text_c10_convert\\\\002.Laysan_Albatross',\n",
      " '.\\\\text_c10_convert\\\\003.Sooty_Albatross',\n",
      " '.\\\\text_c10_convert\\\\004.Groove_billed_Ani',\n",
      " '.\\\\text_c10_convert\\\\005.Crested_Auklet']\n",
      "['.\\\\text_c10_convert\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.txt',\n",
      " '.\\\\text_c10_convert\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0002_55.txt',\n",
      " '.\\\\text_c10_convert\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0003_796136.txt',\n",
      " '.\\\\text_c10_convert\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0005_796090.txt',\n",
      " '.\\\\text_c10_convert\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0006_796065.txt']\n"
     ]
    }
   ],
   "source": [
    "# Converted file path\n",
    "convert_dir_path = \".\\\\text_c10_convert\"\n",
    "\n",
    "convert_dir_list = [os.path.join(convert_dir_path, p ) for p in orig_dir_list]\n",
    "convert_text_list = [ os.path.join(\n",
    "                                    os.path.join(\n",
    "                                        os.path.join(p.split(os.sep)[0], p.split(os.sep)[1] + '_convert')\n",
    "                                    ), \n",
    "                      os.path.join(p.split(os.sep)[2], p.split(os.sep)[3])) for p in orig_text_list] \n",
    "pprint.pprint(convert_dir_list[:5])\n",
    "pprint.pprint(convert_text_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create converted file folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(dir_path):\n",
    "    try:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "    except OSError:\n",
    "        print( \"Error : Creating directory. {}\".format(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in convert_dir_list:\n",
    "    create_folder(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Tokenizer and Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\dwkim\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'batch_size': 1000, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\dwkim\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Create Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create Dependency Parser\n",
    "config = {\n",
    "        'processors': 'tokenize,pos,depparse',\n",
    "        'tokenize_pretokenized': True,\n",
    "        'pos_batch_size': 1000\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting files (Tokenizing and Dependency Parsing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th file is converted from .\\text_c10\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.txt to .\\text_c10_convert\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.txt\n",
      "500 th file is converted from .\\text_c10\\010.Red_winged_Blackbird\\Red_Winged_Blackbird_0027_4123.txt to .\\text_c10_convert\\010.Red_winged_Blackbird\\Red_Winged_Blackbird_0027_4123.txt\n",
      "1000 th file is converted from .\\text_c10\\019.Gray_Catbird\\Gray_Catbird_0007_20186.txt to .\\text_c10_convert\\019.Gray_Catbird\\Gray_Catbird_0007_20186.txt\n",
      "1500 th file is converted from .\\text_c10\\027.Shiny_Cowbird\\Shiny_Cowbird_0059_24421.txt to .\\text_c10_convert\\027.Shiny_Cowbird\\Shiny_Cowbird_0059_24421.txt\n",
      "2000 th file is converted from .\\text_c10\\036.Northern_Flicker\\Northern_Flicker_0034_28740.txt to .\\text_c10_convert\\036.Northern_Flicker\\Northern_Flicker_0034_28740.txt\n",
      "2500 th file is converted from .\\text_c10\\044.Frigatebird\\Frigatebird_0068_42795.txt to .\\text_c10_convert\\044.Frigatebird\\Frigatebird_0068_42795.txt\n",
      "3000 th file is converted from .\\text_c10\\052.Pied_billed_Grebe\\Pied_Billed_Grebe_0106_35418.txt to .\\text_c10_convert\\052.Pied_billed_Grebe\\Pied_Billed_Grebe_0106_35418.txt\n",
      "3500 th file is converted from .\\text_c10\\061.Heermann_Gull\\Heermann_Gull_0034_45693.txt to .\\text_c10_convert\\061.Heermann_Gull\\Heermann_Gull_0034_45693.txt\n",
      "4000 th file is converted from .\\text_c10\\069.Rufous_Hummingbird\\Rufous_Hummingbird_0109_60021.txt to .\\text_c10_convert\\069.Rufous_Hummingbird\\Rufous_Hummingbird_0109_60021.txt\n",
      "4500 th file is converted from .\\text_c10\\078.Gray_Kingbird\\Gray_Kingbird_0010_70057.txt to .\\text_c10_convert\\078.Gray_Kingbird\\Gray_Kingbird_0010_70057.txt\n"
     ]
    }
   ],
   "source": [
    "# Load text files\n",
    "for idx, (orig_text_path, convert_text_path) in enumerate(zip(orig_text_list, convert_text_list)):\n",
    "    if idx % 500 == 0:\n",
    "        print('{} th file is converted from {} to {}'.format(idx, orig_text_path, convert_text_path))\n",
    "        \n",
    "    with open(orig_text_path, 'r') as f:\n",
    "        txt = f.readlines()\n",
    "        \n",
    "# Tokenize the text & Dependency parsing\n",
    "    tokenized_texts = [tokenizer.tokenize(s) for s in txt]\n",
    "    doc = nlp(tokenized_texts)\n",
    "    doc_dep = \"\\n\".join(\n",
    "                    [doc.sentences[i].dependencies_string().replace(\"\\n\", '|')\n",
    "                     for i in range(len(tokenized_texts))]\n",
    "                       )\n",
    "    \n",
    "#     doc_dep = [doc.sentences[i].dependencies_string() for i in range(len(tokenized_texts))]\n",
    "# \n",
    "\n",
    "# Save\n",
    "    with open(convert_text_path, 'w') as f:\n",
    "        f.write(doc_dep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('exercise': conda)",
   "language": "python",
   "name": "python36864bitexercisecondae1983fce3a4241069b6037e09900302b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
