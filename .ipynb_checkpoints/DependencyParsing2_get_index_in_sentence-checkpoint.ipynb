{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing CUB, Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch == 1.1.0\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pprint\n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default download directory: C:\\Users\\dwkim\\Documents\\GitHub\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\dwkim\\Documents\\GitHub\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 235M/235M [01:36<00:00, 2.44MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: C:\\Users\\dwkim\\Documents\\GitHub\\stanfordnlp_resources\\en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross',\n",
      " '002.Laysan_Albatross',\n",
      " '003.Sooty_Albatross',\n",
      " '004.Groove_billed_Ani',\n",
      " '005.Crested_Auklet']\n",
      "['.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0002_55.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0003_796136.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0005_796090.txt',\n",
      " '.\\\\text_c10\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0006_796065.txt']\n"
     ]
    }
   ],
   "source": [
    "# Origin path\n",
    "orig_dir_path = \".\\\\text_c10\"\n",
    "orig_text_path = \".\\\\text_c10\\\\*\\\\*.txt\"\n",
    "orig_dir_list = os.listdir(orig_dir_path)\n",
    "orig_text_list = glob(orig_text_path)\n",
    "pprint.pprint(orig_dir_list[:5])\n",
    "pprint.pprint(orig_text_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross',\n",
      " '.\\\\text_c10_convert_idx\\\\002.Laysan_Albatross',\n",
      " '.\\\\text_c10_convert_idx\\\\003.Sooty_Albatross',\n",
      " '.\\\\text_c10_convert_idx\\\\004.Groove_billed_Ani',\n",
      " '.\\\\text_c10_convert_idx\\\\005.Crested_Auklet']\n",
      "['.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.txt',\n",
      " '.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0002_55.txt',\n",
      " '.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0003_796136.txt',\n",
      " '.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0005_796090.txt',\n",
      " '.\\\\text_c10_convert_idx\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0006_796065.txt']\n"
     ]
    }
   ],
   "source": [
    "# Converted file path\n",
    "convert_dir_path = \".\\\\text_c10_convert_idx\"\n",
    "\n",
    "convert_dir_list = [os.path.join(convert_dir_path, p ) for p in orig_dir_list]\n",
    "convert_text_list = [ os.path.join(\n",
    "                                    os.path.join(\n",
    "                                        os.path.join(p.split(os.sep)[0], p.split(os.sep)[1] + '_convert_idx')\n",
    "                                    ), \n",
    "                      os.path.join(p.split(os.sep)[2], p.split(os.sep)[3])) for p in orig_text_list] \n",
    "pprint.pprint(convert_dir_list[:5])\n",
    "pprint.pprint(convert_text_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bin_list = []\n",
    "for p in convert_text_list:\n",
    "    p = p.split(\".txt\")\n",
    "    p[-1] = \".bin\"\n",
    "    p = \"\".join(p)\n",
    "    convert_bin_list.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create converted file folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(dir_path):\n",
    "    try:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "    except OSError:\n",
    "        print( \"Error : Creating directory. {}\".format(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in convert_dir_list:\n",
    "    create_folder(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Tokenizer and Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dwkim\\Anaconda3\\envs\\exercise\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\Documents\\\\GitHub\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\Documents\\\\GitHub\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\dwkim\\\\Documents\\\\GitHub\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'batch_size': 1000, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\dwkim\\\\Documents\\\\GitHub\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\dwkim\\\\Documents\\\\GitHub\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Create Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create Dependency Parser\n",
    "config = {\n",
    "        'processors': 'tokenize,pos,depparse',\n",
    "        'tokenize_pretokenized': True,\n",
    "        'pos_batch_size': 1000\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting files (Tokenizing and Dependency Parsing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text_list.sort()\n",
    "convert_bin_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th file is converted from .\\text_c10\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.txt to .\\text_c10_convert_idx\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [00:48, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 th file is converted from .\\text_c10\\010.Red_winged_Blackbird\\Red_Winged_Blackbird_0027_4123.txt to .\\text_c10_convert_idx\\010.Red_winged_Blackbird\\Red_Winged_Blackbird_0027_4123.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [01:38, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 th file is converted from .\\text_c10\\019.Gray_Catbird\\Gray_Catbird_0007_20186.txt to .\\text_c10_convert_idx\\019.Gray_Catbird\\Gray_Catbird_0007_20186.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502it [02:26, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 th file is converted from .\\text_c10\\027.Shiny_Cowbird\\Shiny_Cowbird_0059_24421.txt to .\\text_c10_convert_idx\\027.Shiny_Cowbird\\Shiny_Cowbird_0059_24421.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [03:14,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 th file is converted from .\\text_c10\\036.Northern_Flicker\\Northern_Flicker_0034_28740.txt to .\\text_c10_convert_idx\\036.Northern_Flicker\\Northern_Flicker_0034_28740.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [04:04, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 th file is converted from .\\text_c10\\044.Frigatebird\\Frigatebird_0068_42795.txt to .\\text_c10_convert_idx\\044.Frigatebird\\Frigatebird_0068_42795.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [04:54, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 th file is converted from .\\text_c10\\052.Pied_billed_Grebe\\Pied_Billed_Grebe_0106_35418.txt to .\\text_c10_convert_idx\\052.Pied_billed_Grebe\\Pied_Billed_Grebe_0106_35418.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3500it [05:43, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 th file is converted from .\\text_c10\\061.Heermann_Gull\\Heermann_Gull_0034_45693.txt to .\\text_c10_convert_idx\\061.Heermann_Gull\\Heermann_Gull_0034_45693.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [06:33,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 th file is converted from .\\text_c10\\069.Rufous_Hummingbird\\Rufous_Hummingbird_0109_60021.txt to .\\text_c10_convert_idx\\069.Rufous_Hummingbird\\Rufous_Hummingbird_0109_60021.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4502it [07:25, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 th file is converted from .\\text_c10\\078.Gray_Kingbird\\Gray_Kingbird_0010_70057.txt to .\\text_c10_convert_idx\\078.Gray_Kingbird\\Gray_Kingbird_0010_70057.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [08:14, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 th file is converted from .\\text_c10\\086.Pacific_Loon\\Pacific_Loon_0040_75414.txt to .\\text_c10_convert_idx\\086.Pacific_Loon\\Pacific_Loon_0040_75414.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5502it [09:03, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500 th file is converted from .\\text_c10\\094.White_breasted_Nuthatch\\White_Breasted_Nuthatch_0129_86761.txt to .\\text_c10_convert_idx\\094.White_breasted_Nuthatch\\White_Breasted_Nuthatch_0129_86761.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [09:51, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 th file is converted from .\\text_c10\\103.Sayornis\\Sayornis_0066_98309.txt to .\\text_c10_convert_idx\\103.Sayornis\\Sayornis_0066_98309.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [10:41, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500 th file is converted from .\\text_c10\\111.Loggerhead_Shrike\\Loggerhead_Shrike_0128_105238.txt to .\\text_c10_convert_idx\\111.Loggerhead_Shrike\\Loggerhead_Shrike_0128_105238.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [11:30, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 th file is converted from .\\text_c10\\120.Fox_Sparrow\\Fox_Sparrow_0078_114582.txt to .\\text_c10_convert_idx\\120.Fox_Sparrow\\Fox_Sparrow_0078_114582.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [12:18, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 th file is converted from .\\text_c10\\128.Seaside_Sparrow\\Seaside_Sparrow_0066_120791.txt to .\\text_c10_convert_idx\\128.Seaside_Sparrow\\Seaside_Sparrow_0066_120791.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [13:08, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 th file is converted from .\\text_c10\\137.Cliff_Swallow\\Cliff_Swallow_0035_133097.txt to .\\text_c10_convert_idx\\137.Cliff_Swallow\\Cliff_Swallow_0035_133097.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8502it [13:57,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 th file is converted from .\\text_c10\\145.Elegant_Tern\\Elegant_Tern_0072_150911.txt to .\\text_c10_convert_idx\\145.Elegant_Tern\\Elegant_Tern_0072_150911.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [14:47, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 th file is converted from .\\text_c10\\154.Red_eyed_Vireo\\Red_Eyed_Vireo_0023_156800.txt to .\\text_c10_convert_idx\\154.Red_eyed_Vireo\\Red_Eyed_Vireo_0023_156800.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9502it [15:36, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500 th file is converted from .\\text_c10\\162.Canada_Warbler\\Canada_Warbler_0064_162417.txt to .\\text_c10_convert_idx\\162.Canada_Warbler\\Canada_Warbler_0064_162417.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [16:26, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 th file is converted from .\\text_c10\\170.Mourning_Warbler\\Mourning_Warbler_0074_795367.txt to .\\text_c10_convert_idx\\170.Mourning_Warbler\\Mourning_Warbler_0074_795367.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10502it [17:14, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500 th file is converted from .\\text_c10\\179.Tennessee_Warbler\\Tennessee_Warbler_0031_174802.txt to .\\text_c10_convert_idx\\179.Tennessee_Warbler\\Tennessee_Warbler_0031_174802.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11002it [18:04, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 th file is converted from .\\text_c10\\187.American_Three_toed_Woodpecker\\American_Three_Toed_Woodpecker_0040_796180.txt to .\\text_c10_convert_idx\\187.American_Three_toed_Woodpecker\\American_Three_Toed_Woodpecker_0040_796180.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11501it [18:54, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11500 th file is converted from .\\text_c10\\196.House_Wren\\House_Wren_0035_187708.txt to .\\text_c10_convert_idx\\196.House_Wren\\House_Wren_0035_187708.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11788it [19:22, 10.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load text files\n",
    "for idx, (orig_text_path, convert_bin_path) in tqdm.tqdm(enumerate(zip(orig_text_list, convert_bin_list))):\n",
    "    if idx % 500 == 0:\n",
    "        print('{} th file is converted from {} to {}'.format(idx, orig_text_path, convert_bin_path))\n",
    "        \n",
    "    with open(orig_text_path, 'r') as f:\n",
    "        txt = f.readlines()\n",
    "        \n",
    "# Tokenize the text & Dependency parsing\n",
    "    tokenized_texts = [tokenizer.tokenize(s) for s in txt]\n",
    "    doc = nlp(tokenized_texts)\n",
    "    sample = np.array([np.array([np.array([int(i[2].index), i[2].governor]) for i in j.dependencies]) for j in doc.sentences])\n",
    "    \n",
    "#     doc_dep = [doc.sentences[i].dependencies_string() for i in range(len(tokenized_texts))]\n",
    "# \n",
    "# Save\n",
    "    with open(convert_bin_path, 'wb') as f:\n",
    "        pickle.dump(sample, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save it as single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in convert_bin_list:\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 2,  3],\n",
       "       [ 3,  4],\n",
       "       [ 4,  0],\n",
       "       [ 5,  7],\n",
       "       [ 6,  7],\n",
       "       [ 7,  4],\n",
       "       [ 8, 10],\n",
       "       [ 9, 10],\n",
       "       [10,  7],\n",
       "       [11, 10],\n",
       "       [12, 13],\n",
       "       [13, 11],\n",
       "       [14, 13]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathndata = {sample[0].split(os.sep)[-1]:sample[1]  for sample in zip(convert_bin_list, all_data) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11788"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dp_index_dict.bin', 'wb') as f:\n",
    "    pickle.dump(pathndata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('exercise': conda)",
   "language": "python",
   "name": "python36864bitexercisecondae1983fce3a4241069b6037e09900302b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
